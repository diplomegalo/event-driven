# Integrating Event-Driven Architectures With Existing Systems

Le passage à une architecture orientée événements consiste notamment à migrer les données vers un _event broker_ pour qu'il deviennent la source de vérité (_sourcing_), ce processus s'appelle _data liberation_.

Pour les applications ne pouvant pas être migrées vers une architecture orientée événements pour des raisons diverses, il est nécessaire de produire des registres de données courants (créé à partir des événements), à l'instar d'une base de données (_sinking_).

Il existe plusieurs pattern et frameworks pour faire du _sourcing_ et du _sinking_.

## What is Data Liberation?

La libération de données est une partie de la stratégie de migration qui identifie et publie les données vers un _event broker_. Les données publiées sont celles qui sont nécessaires à plusieurs applications et qui sont utilisées pour la communication entre les services. Avec la libération de données, les principes de source de vérité et de découplage sont respectés. De même, les nouveaux services peuvent être construits en consommant les données publiées sur le stream "libéré".

### Compromises for Data Liberation

Une fois les données migrées le stream est maintenu pour garantir la cohérence des données et est ensuite matérialisé en table et mis à disposition des applications. Néanmoins, toutes les applications n'ont pas la possibilité de migrer vers une architecture orientée événements et lire l'event stream comme source de vérité. Dans ce cas, le pattern est inversé et les données du stream sont maintenues à jour sur base des opérations de création et mise à jour des données internes (les données matérialisées).

### Converting Liberated Data to Events

Les données libérées doivent être soumises aux mêmes recommandations de schema et de versioning que les événements.

## Data Liberation Patterns

Il existe plusieurs patterns pour la libération de données:

- **Query-based** : les données sont extraites de la base de données via des requêtes.
- **Log-based** : les données sont extraites sur base des logs (append-only). Ce pattern n'est possible que pour les applications ou base de donnée qui logguent toutes les opérations.
- **Table-based** : dans ce pattern, les données sont d'abord poussé dans la DB puis ensuite envoyées sur le stream. Pour ce cas, il faut que la base de données supporte les transactions et les mécanismes de queues, ou bien un consommateur capable de comprendre une table de _change data _capture_, ou encore via une _outbox_.

Le point commun ici est que les données poussées sur le stream le sont avec la valeur "updated_at" qui correspond à la date de dernière modification et non la date de publication. De cette manière la cohérence des données est garantie dans le stream.

## Data Liberation Frameworks

Il existe plusieurs frameworks pour la libération de données qui peuvent être utilisé pour se connecter à des bases de données et publier les données sur un stream. Néanmoins ces frameworks exposent des données internes en externe, ce qui est un anti-pattern et peut présenté des risques de sécurité. Par conséquent, la solution la plus sûre est de créer un service dédié pour la libération de données.

## Liberating Data by Query

Cette méthode implique l'utilisation d'un client pour se connecter à un base et en extraire les données vers le stream.

### Bulk Loading

Charge toutes les données de manière groupées dans le stream. Cette méthode est adaptée lorsque toutes les données doivent être chargées dans le stream. Les chargements par lots peuvent être effectués à intervalles réguliers ainsi que lors des mises à jours incrémentales des données (ce qui préconise le chargement par lots de l'historique des données). Le chargement par lot peut être très coûteux en termes de ressources et de temps, en effet chargé toutes les données à chaque fois implique l'immobilisation de celles-ci pendant le chargement. Cette méthode est donc à éviter pour les applications nécessitant une faible latence ou une grande quantité de données.

### Incremental Timestamp Loading

Le chargement incrémental des données se limite les dernières données modifiées depuis la dernière exécution, basé sur une colonne de type _updated_at_. Par conséquent, lors de chaque chargement, seules les données avec une date de modification plus récente que la dernière date de chargement sont extraites.

### Autoincrementing ID Loading

Cette méthode est similaire à la précédente, mais utilise une colonne de type _id_ auto-incrémenté pour déterminer les données à charger. Cette méthode est adaptée aux données immuables, qui ne sont pas modifiées après leur création.

### Custom Querying

Cette méthode est utilisée pour extraire des données à partir de requêtes personnalisées de manière à restreindre les données à charger ou lorsqu'il est nécessaire de combiner plusieurs tables (_join_).

### Incremental Loading

La première étape consiste à déterminer le champs utile à la détection des données modifiées. Ensuite la fréquence de chargement des données. Au plus les données seront chargées fréquemment, au plus la latence sera faible. Attention toute fois, dans le cas de grande quantité de données, d'avoir des intervalles de chargement suffisamment long pour s'assurer que les données chargées ont pu l'être totalement avant la prochaine exécution, au risque de se retrouver avec des races conditions. Cette méthode nécessite un bulk loading initial pour charger l'historique des données.

### Benefits of Query-Based Updating

- **Customizable** : n'importe quel _data store_ peut être utilisé dans le champs complet des options de requêtes.
- **Independent polling period** : chaque query peut être exécutée à une fréquence différente selon les SLA et les performances.
- **Isolation of internal data models** : les bases de données peuvent offrir des vues (matérialisées) pour isoler les données internes des données publiées.

### Drawbacks of Query-Based Updating

- **Required `updated-at` column** : la colonne _updated_at_ est nécessaire pour déterminer les données modifiées.
- **Untraceable hard deletions** : les suppressions de données n'apparaîtront pas dans le résultat de la requête. Ce comportement n'est possible que sur base de flag de suppression `is_deleted`.
- **Brittle dependency between data set schema and output event schema** : les schémas de données doivent être alignés entre la base de données et le stream pour garantir la cohérence des données.
- **Intermittent capture** : les données sont synchronisées par intermittence, donc touts les évènements de modifications entre deux exécutions de la requête ne seront pas capturées.
- **Production resource consumption** : les requêtes consomment des ressources de production, il est donc nécessaire de les optimiser pour éviter les problèmes de performance. Ce problème peut être mitigé en utilisant des replicas, mais cela peut augmenter les coûts.
- **Variable query performance due to data changes** : la quantité de données chargées peut varier en fonction des données modifiées. Dans le pire des cas, l'entierté de la base de données peut être récupérée. Cela peut amener à des problèmes de performance et de race conditions lorsque les données n'ont pas encore le temps d'être chargées avant la prochaine exécution de la requête.

## Liberating Data Using Change-Data Capture Logs

Un autre pattern pour la libération de données est l'utilisation des logs de changement de données des bases de données. Ces logs contiennent toutes les opérations effectuées sur la base de données en mode _append-only_. Toutes les bases de données ne possèdent pas de logs de transaction. Généralement une étape préalable de _bootstrapping_ est nécessaire pour charger l'historique des données dans le stream. Cette méthode implique la mise en place d'un _checkpoint_ pour suivre les données déjà chargées. Certains système offre des mécanismes built-in, d'autres nécessitent l'implémentation de mécanismes custom. SQL Server offre par exemple le _Change Data Capture_ (CDC) qui permet de capturer les changements de données dans des tables (et pas des logs).

### Benefits of Using Data Store Logs

- **Delete tracking** : les suppressions de données sont également capturées (contrairement à la méthode de requête).
- **Minimal effect on data store performance** : les logs de changement de données sont généralement écrits de manière asynchrone et n'ont pas d'impact sur les performances de la base de données. Néanmoins, il peut subsister des problèmes dûs à la taille des logs.
- **Low-latency updates** : les données sont publiées dès qu'elles sont écrites dans le journal de transaction.

### Drawbacks of Using Data Store Logs

- **Exposure of internal data models** : les logs de changement de données exposent les données internes de la base de données.
- **Denormalization outside of the data store** : certains systèmes ne permettent pas de dénormaliser les données, par conséquent les streams contiennent des données normalisées avec beaucoup d'identifiants et de jointures. Ce sont donc les applications consommatrices qui doivent dénormaliser les données.
- **Brittle dependency between data set schema and output event schema** : les schémas de données doivent être alignés entre la base de données et le stream pour garantir la cohérence des données.

## Liberating Data Using Outbox Tables

Les tables _outbox_ contiennent les données à publier sur le stream sur base des modifications importantes de la base de données. Toutes opérations de création, mise à jour ou suppression sont enregistrées ont un enregistrement associé dans la table _outbox_ tel que le sont les événements. Il peut y avoir plusieurs tables _outbox_ pour chaque table de la base de données ou une seule table pour toutes les tables de la base de données.

Il est important de bien gérer les transactions pour éviter les incohérences entre les tables, les _outbox_ et le stream. Pour ce faire l'utilisation d'un client transactionnel est recommandé.

Les enregistrements dans les tables _outbox_ doivent avoir des identifiants ordonnés spécifique pour garantir l'ordre des événements, comme un id auto-incrémenté de même qu'un champs _created_at_ pour déterminer la date de création de l'événement.

Les enregistrements dans les tables _outbox_ sont supprimés une fois que l'événement est publié sur le stream.

### Performance Considerations

Les tables _outbox_ ajoute une charge tant en écriture lors de l'ajout des enregistrements dans la table _outbox_ qu'en lecture lorsque le composant de libération de données lit les enregistrements pour les publier sur le stream. Ces opérations peuvent avoir un impact sur les performances de la base de données a fortiori pour les bases de données à forte charge.

### Isolating Internal Data Models

Les tables _outbox_ permettent d'isoler les données internes des données publiées. Les tables _outbox_ peuvent être utilisées pour dénormaliser les données et les rendre plus facilement consommables par les applications.

Un modèle de données créé sur base d'un grand nombre de tables relationnelles peut être complexe, chaque table ayant un stream indépendant, les consommateurs doivent donc être capables de reconstituer les données à partir des différents streams, ce qui ajoute un énorme overhead sur les opérations de lecture. Dans ce cas, soit les tables _outbox_ doivent être dénormalisées, mais cela peut être coûteux en termes de ressources (plus de stockage, plus de CPU, plus de mémoire), soit il faut un nouveau composant consommateur des différents streams pour reconstituer les données et les publier sur un nouveau stream (_eventification_).

### Ensuring Schema Compatibility

La sérialisation et donc la validation peut faire partie intégrante des processus d'extraction des données de manière à injecter des données valides dans les tables _outbox_ et prévenir les erreurs de synchronisation entre elles et les streams. Ainsi les données sont injectées dans les tables _outbox_ et les streams en évitant les incohérences. Néanmoins, cette méthode peut être coûteuse en termes de performance lorsque la charge est élevée.

#### Benefits of event-production with outbox tables

- **Multilanguage support** : cette méthode est supporté par la plupart des client et frameworks faisant du transactionnel.
- **Before-the-fact schema enforcement** : les données sont validées avant d'être injectées dans les tables _outbox_.
- **Isolation of internal data models** : les tables _outbox_ permettent d'isoler les données internes des données publiées.
- **Denormalization** : Les développeurs peuvent dénormaliser les données pour les rendre plus facilement consommables par les applications.

#### Drawbacks of event-production with outbox tables

- **Required application changes** : les applications doivent être modifiées pour écrire dans les tables _outbox_.
- **Business process performance impact** : les opérations d'écriture et de lecture peuvent impacter les performances de la base de données particulièrement lors de la validation des schémas (sérialisation). Dans ce contexte, les transactions d'écriture en erreur peuvent être bloquantes et impacter les processus métiers.
- **Data store performance impact** : les opérations d'écriture et de lecture peuvent impacter les performances de la base de données.

### Capturing Change-Data Using Trigger

Bien avant que la fonctionnalité des logs de changement de données soit disponible dans certains systèmes, les _triggers_ étaient utilisés pour capturer les changements de données. Ce qui fait de ce type de méthode, l'une des plus adapté pour les (très) anciens systèmes. Les _triggers_ sont des procédures stockées qui sont exécutées automatiquement lorsqu'une opération de modification est effectuée sur une table et qui garantissent l'atomicité des opérations. Néanmoins il est presque impossible de valider le schéma de données (sérialisation) dans les _triggers_. Dans certains cas, il est possible d'écrire des fonctions dans un language spécifique à intégrer, mais toutes les bases de données ne supportent pas le multi-language. En outre, les _triggers_ peuvent simplement être trop coûteux en termes de performance, car ils sont déclenchés à chaque opération et demandent une quantité de ressources supplémentaire non négligeable.

Le fait de ne pas pouvoir valider le schéma de données dans les _triggers_, amène au scénario de validation _after-the-fact_ qui demande dès lors une intervention humaine pour corriger les erreurs. Dans cette configuration, la validation et le _push_ des données dans le stream se fait dans un composant ayant cette responsabilité. Ce composant garanti la cohérence et la synchronisation des données entre les tables _outbox_ et les streams.

S'il est vrai que la validation _after-the-fact_ peut être moins optimale, il faut reconnaître que les systèmes pour lesquels seul la méthode de captation par _triggers_ est possible, sont souvent des systèmes anciens pour lesquels un changement de schéma n'est plus à prévoir et par conséquent la validation _after-the-fact_ peut être suffisante. Ceci dit, la validation _after-the-fact_ est donc à éviter pour les systèmes pour lesquels un changement de schéma est à prévoir.

#### Benefits of using triggers

- **Support by most databases** : les _triggers_ sont supportés par la plupart des bases de données.
- **Low overhead for small data sets** : la maintenance et la configuration est assez simple pour les petites bases de données.
- **Customizable logic** : les _triggers_ permettent de définir des logiques personnalisées pour la capture des données, permettant d'isoler les données internes des données publiées.

#### Drawbacks of using triggers

- **Performance overhead** : les _triggers_ peuvent avoir un impact sur les performances de la base de données.
- **Change management complexity** : les maintenances et l'évolution des triggers suite à des modifications peuvent être négligées menant à un processus de _data liberation_ incorrect. Les _triggers_ doivent être testés et maintenus régulièrement.
- **Poor scaling** : les _triggers_ sont intrinsèquement liés aux _data sets_ à remonter, ce qui exclut les triggers existant dans le cadre de la _business logic_.
- **After-the-fact schema enforcement** : la validation du schéma de données se fait après l'insertion des données dans les tables _outbox_.

## Making Data Definition Changes to Data Sets Under Capture

Les changements de _data definition_ font référence aux opérations de DDL (Data Definition Language) qui modifient la structure des tables. Ces changements ont un impact sur la capture des _change data_ et sont complexe à reporter dans le processus de libération de données. La méthode de répercussion dépend du type de pattern utilisé pour la libération de données et plus spécifiquement si les modification de définition de données sont impactante _after-the-fact_ ou _before-the-fact_ lors de la capture des _change data_.

### Handling After-the-Fact Data Definition Changes for the Query and CDC Log Patterns

En ce qui concerne le pattern de query, le schéma est défini par la query et le schéma de l'événement peut en être déduit. C'est ce dernier qui sera validé par rapport au schéma du stream.

En ce qui concerne le pattern de CDC, le schéma est défini par le log de changement de données. Une fois que la modification de la définition est faite, le schéma de log est modifié, il peut être validé par rapport au schéma du stream.

Il existe des frameworks qui permettent de gérer les changements de définition de données pour ces deux cas de figure.

### Handling After-the-Fact Data Definition Changes for Change-Data Table Capture

Les tables de _change data_ agissent comme un _bridge_ entre le schéma des données internes et le schéma des données publiées sur le stream. Tout incompatibilité sera détectée dans le code de validation du composant de libération de données que ce soit un _trigger_ ou un composant dédié, ceci impliquant une intervention qui limitera la propagation de l'erreur.

## Sinking Event Data to Data Stores

Le _sinking_ est le processus inverse du _sourcing_, c'est-à-dire la réintégration des données du stream dans un _data store_. Ce processus peut être réalisé soit par un framework dédié (et centralisé), soit par un microservice dédié (et distribué). N'importe quel type d'événements (entity, à clé ou sans) peut être _sinké_.

Cette opération est nécessaire pour les applications qui ne peuvent pas être migrées vers une architecture orientée événements. Les données sont matérialisées dans un _data store_ pour être lues par les applications. Une application typique de _sinking_ est le remplacement d'un couplage _point-to-point_ entre deux systèmes _legacy_.

Il existe des solutions toute faite pour le _sinking_, dans le cas contraire, il est possible de créer un service dédié pour le _sinking_.

## The impact of Sinking and Sourcing on a Business

Un framework centralisé ayant la responsabilité de la libération de données permet de transférer l'overhead des consomauteurs vers celui-ci. Ce framework peut être gérer par un équipe dédiée qui a comme mission de garantir la cohérence des données, de gérer les changements de schéma et de gérer les erreurs de validation. Par conséquent, les équipes n'ont pas toutes la responsabilité de gérer la migration des données, ce qui dans une grande organisation peut être un avantage. Néanmoins ce type d'approche peut mener à deux pièges. 

Le premier consiste à déresponsabiliser les équipes, les tenants à l'écart des problèmes de migration des données. Ceci ayant pour impact des modifications ayant un impact sur les données sans que l'équipe en gestion du framework soit au courant. Cette situation demande donc beaucoup de concertation entre les équipes.

Le deuxième piège vient de la résistance des équipes à intégrer véritablement le mindset de l'architecture orientée événements en utilisant en premier lieu les techniques de _sinking_ et de _sourcing_ pour leurs applications.

Ces deux pièges peuvent être évités en mettant en place des processus de communication et de collaboration entre les équipes de même qu'en ayant des politiques de réduction d'utilisation du framework de _sinking_ et de _sourcing_. Une autre solution plus radicale serait de décentraliser le framework de _sinking_ et de _sourcing_ en le distribuant dans les équipes, les responsabilisant directement en ce qui concerne la publication des événements. Cela ayant pour effet de ne plus avoir de dépendance entre les équipes et le framework de _sinking_ et de _sourcing_.

En tous les cas, minimiser la dépendance à un framework centralisé est une bonne pratique pour adopter une architecture orientée événements. Les systèmes en mode maintenance peuvent être éventuellement maintenus par ce biais, mais les systèmes encore très actif ayant un impact sur le business doivent être migrés vers une architecture orientée événements.

Il est important également de bien mesurer les compromis entre les différentes stratégies de capture des données. La migration vers une architecture orientée événements demande des investissements dans la couche de gestion des données (data communication layer) et la qualité de celle-ci est inhérente à la qualité des données qui la compose. Chaque migration doit être pensée de manière à mesurer les impacts de la libération de données en termes de SLA concernant les schémas, les modèle de données, la chronologie des événements, les latences et la cohérence des événements produits.

## Summary

La libération de données est une étape importante dans la migration vers une architecture orientée événements. Les stratégies sont multiples et dépendent des contraintes et des politiques d'une organisation. L'objectif est de pouvoir mettre en place une _single source of truth_ et ainsi découpler l'accès aux données de la production et du stockage de celle-ci. Pour ce faire, les frameworks et les stratégies sont multiples. Il revient alors de pouvoir placer le curseur entre une solution qui, d'un côté, responsabilise le producteur de données où celui-ci met en place lui-même les mécanismes de libération de données, en émettant lui-même les événements, en mettant en place des validations de schéma basé sur des discussions avec le consommateur, en évitant les _breaking changes_ dûs à des changements de schéma. De l'autre côté, une solution rapide à mettre en place, avec un framework centralisé qui gère complètement la libération de données, mais qui est difficilement maintenable à long terme.
