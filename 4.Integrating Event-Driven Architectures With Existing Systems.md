# Integrating Event-Driven Architectures With Existing Systems

Le passage à une architecture orientée événements consiste notamment à migrer les données vers un _event broker_ pour qu'il deviennent la source de vérité (_sourcing_), ce processus s'appelle _data liberation_.

Pour les applications ne pouvant pas être migrées vers une architecture orientée événements pour des raisons diverses, il est nécessaire de produire des registres de données courants (créé à partir des événements), à l'instar d'une base de données (_sinking_).

Il existe plusieurs pattern et frameworks pour faire du _sourcing_ et du _sinking_.

## What is Data Liberation?

La libération de données est une partie de la stratégie de migration qui identifie et publie les données vers un _event broker_. Les données publiées sont celles qui sont nécessaires à plusieurs applications et qui sont utilisées pour la communication entre les services. Avec la libération de données, les principes de source de vérité et de découplage sont respectés. De même, les nouveaux services peuvent être construits en consommant les données publiées sur le stream "libéré".

### Compromises for Data Liberation

Une fois les données migrées le stream est maintenu pour garantir la cohérence des données et est ensuite matérialisé en table et mis à disposition des applications. Néanmoins, toutes les applications n'ont pas la possibilité de migrer vers une architecture orientée événements et lire l'event stream comme source de vérité. Dans ce cas, le pattern est inversé et les données du stream sont maintenues à jour sur base des opérations de création et mise à jour des données internes (les données matérialisées).

### Converting Liberated Data to Events

Les données libérées doivent être soumises aux mêmes recommandations de schema et de versioning que les événements.

## Data Liberation Patterns

Il existe plusieurs patterns pour la libération de données:

- **Query-based** : les données sont extraites de la base de données via des requêtes.
- **Log-based** : les données sont extraites sur base des logs (append-only). Ce pattern n'est possible que pour les applications qui logguent toutes les opérations.
- **Table-based** : dans ce pattern, les données sont d'abord poussé dans la DB puis ensuite envoyées sur le stream. Pour ce cas, il faut que la base de données supporte les transactions et les mécanismes de queues.

Le point commun ici est que les données poussées sur le stream le sont avec la valeur "updated_at" qui correspond à la date de dernière modification et non la date de publication. De cette manière la cohérence des données est garantie dans le stream.

## Data Liberation Frameworks

Il existe plusieurs frameworks pour la libération de données qui peuvent être utilisé pour se connecter à des bases de données et publier les données sur un stream. Néanmoins ces frameworks exposent des données internes en externe, ce qui est un anti-pattern et peut présenté des risques de sécurité. Par conséquent, la solution la plus sûre est de créer un service dédié pour la libération de données.

## Liberating Data by Query

Cette méthode implique l'utilisation d'un client pour se connecter à un base et en extraire les données vers le stream.

### Bulk Loading

Charge toutes les données de manière groupées dans le stream. Cette méthode est adaptée lorsque toutes les données doivent être chargées dans le stream. Les chargements par lots peuvent être effectués à intervalles réguliers ainsi que lors des mises à jours incrémentales des données (ce qui préconise le chargement par lots de l'historique des données). Le chargement par lot peut être très coûteux en termes de ressources et de temps, en effet chargé toutes les données à chaque fois implique l'immobilisation de celles-ci pendant le chargement. Cette méthode est donc à éviter pour les applications nécessitant une faible latence ou une grande quantité de données.

### Incremental Timestamp Loading

Le chargement incrémental des données se limite les dernières données modifiées depuis la dernière exécution, basé sur une colonne de type _updated_at_. Par conséquent, lors de chaque chargement, seules les données avec une date de modification plus récente que la dernière date de chargement sont extraites.

### Autoincrementing ID Loading

Cette méthode est similaire à la précédente, mais utilise une colonne de type _id_ auto-incrémenté pour déterminer les données à charger. Cette méthode est adaptée aux données immuables, qui ne sont pas modifiées après leur création.

### Custom Querying

Cette méthode est utilisée pour extraire des données à partir de requêtes personnalisées de manière à restreindre les données à charger ou lorsqu'il est nécessaire de combiner plusieurs tables (_join_).

### Incremental Loading

La première étape consiste à déterminer le champs utile à la détection des données modifiées. Ensuite la fréquence de chargement des données. Au plus les données seront chargées fréquemment, au plus la latence sera faible. Attention toute fois, dans le cas de grande quantité de données, d'avoir des intervalles de chargement suffisamment long pour s'assurer que les données chargées ont pu l'être totalement avant la prochaine exécution, au risque de se retrouver avec des races conditions. Cette méthode nécessite un bulk loading initial pour charger l'historique des données.

### Benefits of Query-Based Updating

- **Customizable** : n'importe quel _data store_ peut être utilisé dans le champs complet des options de requêtes.
- **Independent polling period** : chaque query peut être exécutée à une fréquence différente selon les SLA et les performances.
- **Isolation of internal data models** : les bases de données peuvent offrir des vues (matérialisées) pour isoler les données internes des données publiées.

### Drawbacks of Query-Based Updating

- **Required `updated-at` column** : la colonne _updated_at_ est nécessaire pour déterminer les données modifiées.
- **Untraceable hard deletions** : les suppressions de données n'apparaîtront pas dans le résultat de la requête. Ce comportement n'est possible que sur base de flag de suppression `is_deleted`.
- **Brittle dependency between data set schema and output event schema** : les schémas de données doivent être alignés entre la base de données et le stream pour garantir la cohérence des données.
- **Intermittent capture** : les données sont synchronisées par intermittence, donc touts les évènements de modifications entre deux exécutions de la requête ne seront pas capturées.
- **Production resource consumption** : les requêtes consomment des ressources de production, il est donc nécessaire de les optimiser pour éviter les problèmes de performance. Ce problème peut être mitigé en utilisant des replicas, mais cela peut augmenter les coûts.
- **Variable query performance due to data changes** : la quantité de données chargées peut varier en fonction des données modifiées. Dans le pire des cas, l'entierté de la base de données peut être récupérée. Cela peut amener à des problèmes de performance et de race conditions lorsque les données n'ont pas encore le temps d'être chargées avant la prochaine exécution de la requête.

## Liberating Data Using Change-Data Capture Logs

Un autre pattern pour la libération de données est l'utilisation des logs de changement de données des bases de données par exemple le journal de transactions dans SQL Server. Ce journal de transaction contient toutes les opérations effectuées sur la base de données en mode _append-only_. Toutes les base de données ne possèdent pas de journal de transaction. Généralement une étape de _bootstrapping_ est nécessaire pour charger l'historique des données dans le stream. Cette méthode implique la mise en place d'un _checkpoint_ pour suivre les données déjà chargées. Certains système offre des mécanismes built-in, d'autres nécessitent l'implémentation de mécanismes custom.

### Benefits of Using Data Store Logs

- **Delete tracking** : les suppressions de données sont également capturées (contrairement à la méthode de requête).
- **Minimal effect on data store performance** : les logs de changement de données sont généralement écrits de manière asynchrone et n'ont pas d'impact sur les performances de la base de données. Néanmoins, il peut subsister des problèmes dûs à la taille des logs.
- **Low-latency updates** : les données sont publiées dès qu'elles sont écrites dans le journal de transaction.

### Drawbacks of Using Data Store Logs

- **Exposure of internal data models** : les logs de changement de données exposent les données internes de la base de données.
- **Denormalization outside of the data store** : certains systèmes ne permettent pas de dénormaliser les données, par conséquent les streams contiennent des données normalisées avec beaucoup d'identifiants et de jointures. Ce sont donc les applications consommatrices qui doivent dénormaliser les données.
- **Brittle dependency between data set schema and output event schema** : les schémas de données doivent être alignés entre la base de données et le stream pour garantir la cohérence des données.

## Liberating Data Using Outbox Tables

Les tables _outbox_ contiennent les données à publier sur le stream sur base des modifications importantes de la base de données. Toutes opérations de création, mise à jour ou suppression sont enregistrées ont un enregistrement associé dans la table _outbox_. Il peut y avoir plusieurs tables _outbox_ pour chaque table de la base de données ou une seule table pour toutes les tables de la base de données.

Il est important de bien gérer les transactions pour éviter les incohérences entre les tables, les _outbox_ et le stream.

Les enregistrements dans les tables _outbox_ doivent avoir des identifiants ordonnés spécifique pour garantir l'ordre des événements, comme un id auto-incrémenté de même qu'un champs _created_at_ pour déterminer la date de création de l'événement.

Les enregistrements dans les tables _outbox_ sont supprimés une fois que l'événement est publié sur le stream.

### Performance Considerations

Les tables _outbox_ ajoute une charge tant en écriture lors de l'ajout des enregistrements dans la table _outbox_ qu'en lecture lorsque le composant de libération de données lit les enregistrements pour les publier sur le stream. Ces opérations peuvent avoir un impact sur les performances de la base de données a fortiori pour les bases de données à forte charge.

### Isolating Internal Data Models

Les tables _outbox_ permettent d'isoler les données internes des données publiées. Les tables _outbox_ peuvent être utilisées pour dénormaliser les données et les rendre plus facilement consommables par les applications.

Un modèle de données créé sur base d'un grand nombre de tables relationnelles peut être complexe, chaque table ayant un stream indépendant, les consommateurs doivent donc être capables de reconstituer les données à partir des différents streams, ce qui ajoute un énorme overhead sur les opérations de lecture. Dans ce cas, soit les tables _outbox_ doivent être dénormalisées, mais cela peut être coûteux en termes de ressources (plus de stockage, plus de CPU, plus de mémoire), soit il faut un nouveau composant consommateur des différents streams pour reconstituer les données et les publier sur un nouveau stream (_eventification_).

### Ensuring Schema Compatibility

La sérialisation et donc la validation peut faire partie intégrante des processus d'extraction des données de manière à injecter des données valides dans les tables _outbox_ et prévenir les erreurs de synchronisation entre elles et les streams. Ainsi les données sont injectées dans les tables _outbox_ et les streams en une transaction et les incohérences sont évitées ou détectées. Néanmoins, cette méthode peut être coûteuse en termes de performance lorsque la charge est élevée.

